# assets/config/stars.yaml
background_image: "assets/figures/stars.png"
stars:
  - x: 120
    y: 271
    radius: 4
    text: "How should videos be sampled (fps vs. uniform)?"    # Q1
  - x: 176
    y: 328
    radius: 4
    text: "How many frames or tokens per frame are optimal?"   # Q2
  - x: 232
    y: 383
    radius: 4
    text: "How do we handle large video durations?"            # Q3
  - x: 176
    y: 243
    radius: 4
    text: "Which vision encoders yield optimal representations?" # Q4
  - x: 288
    y: 355
    radius: 4
    text: "Should we use video encoders or image encoders?"    # Q5
  - x: 316
    y: 271
    radius: 4
    text: "Should we combine multiple encoders?"               # Q6
  - x: 274
    y: 229
    radius: 4
    text: "What token resampler is best?"                      # Q7
  - x: 344
    y: 215
    radius: 4
    text: "How much can we resample video tokens without losing performance?" # Q8
  - x: 330
    y: 118
    radius: 4
    text: "What data mixture of text/image/video is optimal?"  # Q9
  - x: 372
    y: 75
    radius: 4
    text: "How much text data is needed to prevent catastrophic forgetting?" # Q10
  - x: 456
    y: 159
    radius: 4
    text: "Should the mixture be slightly video-heavy?"         # Q11
  - x: 568
    y: 271
    radius: 4
    text: "Do we need multi-stage training?"                   # Q12
  - x: 680
    y: 383
    radius: 4
    text: "When should we freeze/unfreeze video encoders?"     # Q13
  - x: 638
    y: 425
    radius: 4
    text: "Should we train on video-only first or mix from the start?" # Q14
  - x: 540
    y: 313
    radius: 4
    text: "Should we insert textual timestamps or learned tokens between frames?" # Q15
  - x: 390
    y: 196
    radius: 4
    text: "Do design choices from small models transfer to larger ones?" # Q16
